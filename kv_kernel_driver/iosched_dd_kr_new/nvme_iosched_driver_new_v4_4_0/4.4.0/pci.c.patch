--- pci.c	2019-10-22 13:02:50.780193000 -0700
+++ pci.c.patched	2019-10-22 13:00:40.231772000 -0700
@@ -2,6 +2,10 @@
  * NVM Express device driver
  * Copyright (c) 2011-2014, Intel Corporation.
  *
+ * Modified by Heekwon Park from Samsung Electronics.
+ * See KV_NVME_SUPPORT to check modification.
+ * E-mail : heekwon.p@samsung.com
+ *
  * This program is free software; you can redistribute it and/or modify it
  * under the terms and conditions of the GNU General Public License,
  * version 2, as published by the Free Software Foundation.
@@ -47,7 +51,12 @@
 
 #include "nvme.h"
 
+#ifdef KV_NVME_SUPPORT
+#define NVME_Q_DEPTH		1024
+#else
+/* Original value is 1024 */
 #define NVME_Q_DEPTH		1024
+#endif
 #define NVME_AQ_DEPTH		256
 #define SQ_SIZE(depth)		(depth * sizeof(struct nvme_command))
 #define CQ_SIZE(depth)		(depth * sizeof(struct nvme_completion))
@@ -96,6 +105,7 @@
 /*
  * Represents an NVM Express device.  Each nvme_dev is a PCI function.
  */
+#ifndef KV_NVME_SUPPORT
 struct nvme_dev {
 	struct nvme_queue **queues;
 	struct blk_mq_tag_set tagset;
@@ -136,6 +146,7 @@
 	dma_addr_t eventidx;
 #endif
 };
+#endif
 
 static inline struct nvme_dev *to_nvme_dev(struct nvme_ctrl *ctrl)
 {
@@ -692,6 +703,189 @@
 
 	nvme_free_iod(dev, req);
 }
+#ifdef KV_NVME_SUPPORT
+static int kvs_nvme_map_data(struct nvme_dev *dev, struct request *req,
+		struct nvme_command *cmnd)
+{
+    struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+    struct request_queue *q = req->q;
+    enum dma_data_direction dma_dir = DMA_BIDIRECTIONAL;
+    int ret = BLK_MQ_RQ_QUEUE_ERROR;
+
+    if (req->nr_phys_segments)/*if !delete*/
+    {
+        sg_init_table(iod->sg, req->nr_phys_segments);
+        iod->nents = blk_rq_map_sg(q, req, iod->sg);
+        if (!iod->nents)
+            goto out;
+
+        ret = BLK_MQ_RQ_QUEUE_BUSY;
+	    if (!dma_map_sg_attrs(dev->dev, iod->sg, iod->nents, dma_dir,
+				&nvme_dma_attrs))
+            goto out;
+        if (!nvme_setup_prps(dev, req, blk_rq_bytes(req)))
+            goto out_unmap;
+
+        cmnd->rw.dptr.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
+        cmnd->rw.dptr.prp2 = cpu_to_le64(iod->first_dma);
+    }
+
+    ret = BLK_MQ_RQ_QUEUE_ERROR;
+
+    if (req->bio && bio_integrity(req->bio))
+    {
+
+        if (blk_rq_count_integrity_sg(q, req->bio) != 1)
+            goto out_unmap;
+
+        sg_init_table(&iod->meta_sg, 1);
+        if (blk_rq_map_integrity_sg(q, req->bio, &iod->meta_sg) != 1)
+            goto out_unmap;
+
+        if (!dma_map_sg(dev->dev, &iod->meta_sg, 1, DMA_TO_DEVICE))
+            goto out_unmap;
+        cmnd->kv_store.key_prp = cpu_to_le64(sg_dma_address(&iod->meta_sg));
+
+    } 
+#if 0
+    else if (blk_integrity_rq(req)) {
+            if (blk_rq_count_integrity_sg(q, req->bio) != 1)
+                goto out_unmap;
+
+            sg_init_table(&iod->meta_sg, 1);
+            if (blk_rq_map_integrity_sg(q, req->bio, &iod->meta_sg) != 1)
+                goto out_unmap;
+
+            if (rq_data_dir(req))
+                nvme_dif_remap(req, nvme_dif_prep);
+
+            if (!dma_map_sg(dev->dev, &iod->meta_sg, 1, dma_dir))
+                goto out_unmap;
+            cmnd->rw.metadata = cpu_to_le64(sg_dma_address(&iod->meta_sg));
+        }
+#endif
+    return BLK_MQ_RQ_QUEUE_OK;
+
+out_unmap:
+    if (req->nr_phys_segments)
+        dma_unmap_sg(dev->dev, iod->sg, iod->nents, dma_dir);
+out:
+    return ret;
+}
+
+
+static void kvs_nvme_unmap_data(struct nvme_dev *dev, struct request *req)
+{
+	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+    enum dma_data_direction dma_dir = DMA_BIDIRECTIONAL;
+
+	if (iod->nents) {
+		dma_unmap_sg(dev->dev, iod->sg, iod->nents, dma_dir);
+		if (req->bio && bio_integrity(req->bio))
+			dma_unmap_sg(dev->dev, &iod->meta_sg, 1, dma_dir);
+#if 0 
+        else if(blk_integrity_rq(req)) {
+			if (!rq_data_dir(req))
+				nvme_dif_remap(req, nvme_dif_complete);
+			dma_unmap_sg(dev->dev, &iod->meta_sg, 1, dma_dir);
+		}
+#endif
+	}
+	nvme_free_iod(dev, req);
+}
+
+/*
+ * nvme_queue_rq for kv nvme device
+ */
+static int kvs_nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
+			 const struct blk_mq_queue_data *bd)
+{
+	struct nvme_ns *ns = hctx->queue->queuedata;
+	struct nvme_queue *nvmeq = hctx->driver_data;
+	struct nvme_dev *dev = nvmeq->dev;
+	struct request *req = bd->rq;
+	struct nvme_command cmnd;
+	int ret = BLK_MQ_RQ_QUEUE_OK;
+
+	/*
+	 * If formated with metadata, require the block layer provide a buffer
+	 * unless this namespace is formated such that the metadata can be
+	 * stripped/generated by the controller with PRACT=1.
+	 */
+	if (ns && ns->ms && !blk_integrity_rq(req)) {
+		if (!(ns->pi_type && ns->ms == 8) &&
+					req->cmd_type != REQ_TYPE_DRV_PRIV) {
+			blk_mq_end_request(req, -EFAULT);
+			return BLK_MQ_RQ_QUEUE_OK;
+		}
+	}
+
+	ret = nvme_init_iod(req, dev);
+	if (ret)
+		goto out_free_integrity;
+
+	memcpy(&cmnd, req->cmd, sizeof(cmnd));
+
+#if 0 
+    /* Call nvme_map_data even if nr_phys_segments is 0 
+     * Del cmd may have zero nr_phys_segments.
+     */ 
+	if (req->nr_phys_segments || is_kv_cmd(cmnd.common.opcode))
+#endif
+		ret = kvs_nvme_map_data(dev, req, &cmnd);
+
+	if (ret)
+		goto out;
+
+	cmnd.common.command_id = req->tag;
+	blk_mq_start_request(req);
+
+	spin_lock_irq(&nvmeq->q_lock);
+	if (unlikely(nvmeq->cq_vector < 0)) {
+		if (ns && !test_bit(NVME_NS_DEAD, &ns->flags))
+			ret = BLK_MQ_RQ_QUEUE_BUSY;
+		else
+			ret = BLK_MQ_RQ_QUEUE_ERROR;
+		spin_unlock_irq(&nvmeq->q_lock);
+		goto out;
+	}
+	__nvme_submit_cmd(nvmeq, &cmnd);
+	nvme_process_cq(nvmeq);
+	spin_unlock_irq(&nvmeq->q_lock);
+	return BLK_MQ_RQ_QUEUE_OK;
+out:
+	nvme_free_iod(dev, req);
+out_free_integrity:
+	if (req->cmd_type == REQ_TYPE_DRV_PRIV) {
+		if (req->bio && bio_integrity(req->bio))
+			bio_integrity_free(req->bio);
+	}
+	return ret;
+}
+#endif
+
+
+#ifdef KV_NVME_SUPPORT
+bool is_kv_dev(struct nvme_ctrl *ctrl) {
+    if (ctrl) {
+        if (memcmp(ctrl->firmware_rev, "EEA", 3) == 0 ||
+            memcmp(ctrl->firmware_rev, "ETA", 3) == 0 ||
+            memcmp(ctrl->firmware_rev, "EHA", 3) == 0 ||
+            memcmp(ctrl->firmware_rev, "EFA", 3) == 0 ) {
+            return true;
+        }
+    }
+    return false;
+}
+
+inline void set_request_result(struct request *req, __u32 result)
+{
+    if(req->special)
+        (((struct nvme_completion *)(req->special))->result) = cpu_to_le32(result);
+}
+
+#endif
+
 
 /*
  * We reuse the small pool to allocate the 16-byte range here as it is not
@@ -742,6 +936,11 @@
 	 * unless this namespace is formated such that the metadata can be
 	 * stripped/generated by the controller with PRACT=1.
 	 */
+#ifdef KV_NVME_SUPPORT
+	if (is_kv_cmd(((struct nvme_command *)req->cmd)->common.opcode)) {
+        return kvs_nvme_queue_rq(hctx, bd);
+    }
+#endif
 	if (ns && ns->ms && !blk_integrity_rq(req)) {
 		if (!(ns->pi_type && ns->ms == 8) &&
 					req->cmd_type != REQ_TYPE_DRV_PRIV) {
@@ -773,7 +972,27 @@
 
 	cmnd.common.command_id = req->tag;
 	blk_mq_start_request(req);
+#ifdef KV_NVME_SUPPORT
+    /**
+     * Ignoring Block Command in KV DD
+     */
+    if (ns && is_kv_dev(ns->ctrl)) {
+
+		//pr_err("Non KV request to KV device: opcode (%02x) nsid(%d).\n", cmnd.common.opcode, ((ns) ? ns->ns_id : 0));
+        if (is_kv_retrieve_cmd(cmnd.common.opcode))
+            set_request_result(req, (le32_to_cpu(cmnd.common.cdw10[0]) >> 2));
+        else
+            set_request_result(req, 0);
+
+        blk_mq_complete_request(req, 0);
+        if(req->end_io){
+            printk("end_io called!!\n");
+            req->end_io(req, 0);
+        }
+        return BLK_MQ_RQ_QUEUE_OK;
+    }
 
+#endif
 	spin_lock_irq(&nvmeq->q_lock);
 	if (unlikely(nvmeq->cq_vector < 0)) {
 		if (ns && !test_bit(NVME_NS_DEAD, &ns->flags))
@@ -797,7 +1016,17 @@
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
 	struct nvme_dev *dev = iod->nvmeq->dev;
 	int error = 0;
-
+#ifdef KV_NVME_SUPPORT
+    bool kv_cmd = false;
+    if(req->cmd_type == REQ_TYPE_DRV_PRIV && req->cmd){
+        struct nvme_command *cmd = (struct nvme_command *)req->cmd;
+        if(is_kv_cmd(cmd->common.opcode)){
+            kv_cmd = true;
+            kvs_nvme_unmap_data(dev, req);
+        }
+    }
+    else
+#endif
 	nvme_unmap_data(dev, req);
 
 	if (unlikely(req->errors)) {
@@ -817,6 +1046,10 @@
 			"completing aborted command with status: %04x\n",
 			req->errors);
 	}
+#ifdef KV_NVME_SUPPORT
+    if(kv_cmd && req->bio && bio_integrity(req->bio))
+        bio_integrity_free(req->bio);
+#endif
 
 	blk_mq_end_request(req, error);
 }
@@ -929,8 +1162,10 @@
 	return IRQ_NONE;
 }
 
-static int __nvme_poll(struct nvme_queue *nvmeq, unsigned int tag)
+static int nvme_poll(struct blk_mq_hw_ctx *hctx, unsigned int tag)
 {
+	struct nvme_queue *nvmeq = hctx->driver_data;
+
 	if (nvme_cqe_valid(nvmeq, nvmeq->cq_head, nvmeq->cq_phase)) {
 		spin_lock_irq(&nvmeq->q_lock);
 		__nvme_process_cq(nvmeq, &tag);
@@ -943,13 +1178,6 @@
 	return 0;
 }
 
-static int nvme_poll(struct blk_mq_hw_ctx *hctx, unsigned int tag)
-{
-	struct nvme_queue *nvmeq = hctx->driver_data;
-
-	return __nvme_poll(nvmeq, tag);
-}
-
 static void nvme_async_event_work(struct work_struct *work)
 {
 	struct nvme_dev *dev = container_of(work, struct nvme_dev, async_work);
@@ -1051,16 +1279,6 @@
 	struct nvme_command cmd;
 
 	/*
-	* Did we miss an interrupt?
-	*/
-	if (__nvme_poll(nvmeq, req->tag)) {
-		dev_warn(dev->dev,
-			"I/O %d QID %d timeout, completion polled\n",
-			req->tag, nvmeq->qid);
-		return BLK_EH_HANDLED;
-	}
-
-	/*
 	 * Shutdown immediately if controller times out while starting. The
 	 * reset work will see the pci device disabled when it gets the forced
 	 * cancellation error. All outstanding requests are completed on
